import os
import fitz  # PyMuPDF
from langchain_community.document_loaders import DirectoryLoader, TextLoader, JSONLoader
from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings.ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms.ollama import Ollama
from langchain.chains import RetrievalQA
from langchain_ollama import OllamaEmbeddings  # å¼•å…¥æ–°çš„ OllamaEmbeddings


# æŒ‡å®šè³‡æ–™å¤¾
data_dir = "data"

if not os.path.exists(data_dir):
    print(f"âŒ è³‡æ–™å¤¾ {data_dir} ä¸å­˜åœ¨ï¼Œè«‹ç¢ºèªè³‡æ–™å¤¾åç¨±")
else:
    loader = DirectoryLoader(data_dir, glob="*.*", show_progress=True)

    documents = []

    for filename in os.listdir(data_dir):
        file_path = os.path.join(data_dir, filename)

        if filename.endswith(".json"):
            try:
                json_loader = JSONLoader(file_path=file_path, jq_schema=".[] | {text: .content, metadata: {source: .source}}")
                documents.extend(json_loader.load())
            except Exception as e:
                print(f"âŒ JSON è§£æå¤±æ•— {filename}: {e}")

        elif filename.endswith(".txt") or filename.endswith(".md"):
            try:
                text_loader = TextLoader(file_path)
                documents.extend(text_loader.load())
            except Exception as e:
                print(f"âŒ TXT è§£æå¤±æ•— {filename}: {e}")

        elif filename.endswith(".pdf"):  # **æ–°å¢ PDF è§£æ**
            try:
                with fitz.open(file_path) as doc:
                    text = "\n".join([page.get_text("text") for page in doc])
                    documents.append(Document(page_content=text, metadata={"source": file_path}))
            except Exception as e:
                print(f"âŒ PDF è§£æå¤±æ•— {filename}: {e}")

        else:
            print(f"âš ï¸ ä¸æ”¯æ´çš„æ ¼å¼: {filename}")

    print(f"ğŸ“‚ æœ€çµ‚æˆåŠŸè¼‰å…¥ {len(documents)} ç­†æ–‡ä»¶")

# **1. æ‹†åˆ†æ–‡æœ¬ï¼Œæå‡æª¢ç´¢æ•ˆæœ**
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# **2. å»ºç«‹å‘é‡è³‡æ–™åº« - ä½¿ç”¨ FAISS**
embedding = OllamaEmbeddings(model="deepseek-r1:1.5b")

# FAISS å‘é‡è³‡æ–™åº«
faiss_vectorstore = FAISS.from_documents(docs, embedding)

# **3. å•Ÿå‹• RAG å•ç­”ç³»çµ±**
retriever = faiss_vectorstore.as_retriever()
llm = Ollama(model="deepseek-r1:1.5b")
qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)

if __name__ == "__main__":
    # **4. æ¸¬è©¦ AI å°åŠ©ç†**
    question = "è«‹è§£é‡‹ IFRS 16 ç§Ÿè³ƒæ¨™æº–çš„ä¸»è¦å…§å®¹"
    response = qa_chain.run(question)
    print(response)
